{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPMlo5MYaBH7HGx2lMxifZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Hate speech detection using Machine Learning**"
      ],
      "metadata": {
        "id": "43lGmgtGuran"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk #natural language tool kit\n",
        "import string\n",
        "import re #regular expression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from nltk.util import pr\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "id": "cotlsHx1vA4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the dataset\n",
        "data = pd.read_csv(\"twitter.csv\")\n",
        "print(data.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIO5qu2Fwtpg",
        "outputId": "3588b207-e658-4edc-8765-8ff7e3f30783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
            "0           0      3            0                   0        3      2   \n",
            "1           1      3            0                   3        0      1   \n",
            "2           2      3            0                   3        0      1   \n",
            "3           3      3            0                   2        1      1   \n",
            "4           4      6            0                   6        0      1   \n",
            "5           5      3            1                   2        0      1   \n",
            "6           6      3            0                   3        0      1   \n",
            "7           7      3            0                   3        0      1   \n",
            "8           8      3            0                   3        0      1   \n",
            "9           9      3            1                   2        0      1   \n",
            "\n",
            "                                               tweet  \n",
            "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
            "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
            "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
            "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
            "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
            "5  !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...  \n",
            "6  !!!!!!\"@__BrighterDays: I can not just sit up ...  \n",
            "7  !!!!&#8220;@selfiequeenbri: cause I'm tired of...  \n",
            "8  \" &amp; you might not get ya bitch back &amp; ...  \n",
            "9  \" @rhythmixx_ :hobbies include: fighting Maria...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The nltk.SnowballStemmer(\"english\") is a function in the Natural Language Toolkit (NLTK) library, used for stemming words in the English language.\n",
        "\n",
        "Stemming is the process of reducing words to their base or root form. For instance, the words \"running\", \"runner\", and \"ran\" all stem from the root word \"run\". Stemming helps in normalizing words for various natural language processing (NLP) tasks."
      ],
      "metadata": {
        "id": "3Ot0x4ne14L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = nltk.SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "TGBK9LgF0J5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the columns for hate speech creating a new column called 'label'\n",
        "data[\"label\"] = data[\"class\"].map({0: \"Hate Speech\", 1: \"Offensive Language\", 2: \"No Hate and Offensive\"})\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWtjbeX0z1Qb",
        "outputId": "551c703e-42a1-40a7-c2f0-7b22882ae986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
            "0           0      3            0                   0        3      2   \n",
            "1           1      3            0                   3        0      1   \n",
            "2           2      3            0                   3        0      1   \n",
            "3           3      3            0                   2        1      1   \n",
            "4           4      6            0                   6        0      1   \n",
            "\n",
            "                                               tweet                  label  \n",
            "0  !!! RT @mayasolovely: As a woman you shouldn't...  No Hate and Offensive  \n",
            "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...     Offensive Language  \n",
            "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     Offensive Language  \n",
            "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...     Offensive Language  \n",
            "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     Offensive Language  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split the dataset or have a seperate data so that only required columns or according to our need are there in the newly available.\n",
        "data1 = data[['tweet','label']]\n",
        "print(data1.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEfdQuAl3VL6",
        "outputId": "f5852cfb-ec67-4260-fa7f-f9d9836b2db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               tweet                  label\n",
            "0  !!! RT @mayasolovely: As a woman you shouldn't...  No Hate and Offensive\n",
            "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...     Offensive Language\n",
            "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...     Offensive Language\n",
            "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...     Offensive Language\n",
            "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...     Offensive Language\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The re library in Python provides support for regular expressions, which are a powerful tool for matching patterns in text. Regular expressions allow you to search, match, and manipulate strings based on specific patterns."
      ],
      "metadata": {
        "id": "j1r9m0Jy59B1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords are common words that are usually filtered out during natural language processing (NLP) tasks because they carry little meaningful information about the content of the text. Examples of stopwords include \"and\", \"the\", \"is\", \"in\", and \"at\". Removing stopwords helps to reduce the size of the data and focus on the more important words that are relevant to the analysis."
      ],
      "metadata": {
        "id": "O2WapypkFrLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# Initialize the stemmer and tokenizer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "tokenizer = TweetTokenizer()\n"
      ],
      "metadata": {
        "id": "RGD4HpTbEuNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing is the process of splitting text into individual units, called tokens. These tokens can be words, phrases, symbols, or other meaningful elements. In natural language processing (NLP), tokenizing is a crucial step because it allows the text to be processed and analyzed more easily."
      ],
      "metadata": {
        "id": "wvjmIntfEsWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ra56RYJoGzLw",
        "outputId": "8e93ff72-134c-47a5-c568-ccd0f9689130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clean the sentence or text in dataset\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remove hashtags (keeping the text after the #)\n",
        "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "\n",
        "    # Remove special characters, numbers, and punctuations\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = tokenizer.tokenize(text)\n",
        "\n",
        "    # Remove stopwords and stem the words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "\n",
        "    # Join the cleaned words back into a single string\n",
        "    cleaned_text = ' '.join(words)\n",
        "\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "ChocZUst4Ud9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# applying the function to the data1 which is our new dataset with only the necessarry columns\n",
        "data1['tweet'] = data1['tweet'].apply(clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mu3ovRnGTGR",
        "outputId": "542cc80f-afb5-494d-a29b-6506ba025ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-0d3b75752e22>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data1['tweet'] = data1['tweet'].apply(clean_text)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training using Decision tree classifier**"
      ],
      "metadata": {
        "id": "laYSFh7CIHe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer is a simple yet powerful tool for converting text data into numerical data for machine learning models. It creates a matrix of token counts, which can be further used for various NLP tasks like text classification, clustering, and topic modeling. The flexibility of CountVectorizer allows for easy customization to suit different text processing needs."
      ],
      "metadata": {
        "id": "55MHX_WMJp1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()"
      ],
      "metadata": {
        "id": "JChJUUYQJr0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array(data1['tweet'])\n",
        "y = np.array(data1['label'])\n",
        "\n",
        "X = cv.fit_transform(x)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=123)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(x_train,y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "XPnGmgG7Gt23",
        "outputId": "5f591149-9186-44c8-b14f-252eef8792e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "# Make predictions\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0upmh6osL-QX",
        "outputId": "fcd43385-1223-45a4-8557-4d9425c0bc11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.88\n",
            "\n",
            "Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "          Hate Speech       0.34      0.30      0.32       436\n",
            "No Hate and Offensive       0.82      0.85      0.83      1247\n",
            "   Offensive Language       0.93      0.93      0.93      5752\n",
            "\n",
            "             accuracy                           0.88      7435\n",
            "            macro avg       0.69      0.69      0.69      7435\n",
            "         weighted avg       0.87      0.88      0.88      7435\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "sample = \"It's been a killing journey\"\n",
        "d = cv.transform([sample]).toarray()\n",
        "print(clf.predict(d))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9cM3_7tNTiu",
        "outputId": "28df884d-292d-4a0a-eb3d-72da3e16918c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No Hate and Offensive']\n"
          ]
        }
      ]
    }
  ]
}